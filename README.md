# ADTests

This repository is used to tabulate the current performance of various automatic differentiation (AD) backends with Turing.jl models.

## Let me see the results!

https://turinglang.github.io/ADTests

## I want to add more AD backends!

You can modify the list of AD types in `main.jl`.

## I want to add more models!

You can modify the list of models in `models.jl`.

## I want to edit the HTML that gets generated!

The HTML is generated by the `collate.py` script.
You can edit it there.

Note that the links-to-existing-GitHub-issues in the table are also defined in this script.

## What's going on?

The workflow is the most complicated part of this repository.
This section attempts to explain it from the 'bottom up'; if you prefer a 'top down' approach start by looking at the GitHub Action workflow, `.github/workflows/test.yml`.

Firstly, there is library code for running the benchmarks.
This is in `lib.jl`; it should (in the near future) be put directly into DynamicPPL.jl.
Until then, it has to live here.

Under the hood, the main thing that actually runs the AD tests / benchmarks is `main.jl`.
You can run `julia --project=. main.jl` and it will print some usage information.
However, it is the Bash script `ad.sh` that controls how this Julia script is called.

Fundamentally, the idea is that we want to loop over every combination of model and adtype and test it.
However, because GitHub Actions limits jobs to 6 hours, it is impractical to run every combination in the same job.
What we do is to run one job per model.
This is accomplished by first storing the names of the models and adtypes in the `$GITHUB_OUTPUT` variable, which can then be read by the next job.

The next job is a CI matrix split by model name; each of these invokes `ad.sh` to loop over each adtype.
The purpose of having a Bash script call Julia is to guard against the Julia process crashing, which can happen especially with Enzyme.
If the Julia process successfully finishes, it will print the result which is picked up by the Bash script; if it crashes, we just record the result as 'error'.

Finally, the results are collated and sent to the final job in the workflow, which is a Python script that uses the results to create the HTML page.
It _could_ be written in Julia (PRs are welcome).
However, it was much faster for me to prototype in Python.
(Fun fact: collating these results is also somewhat involved because we can't just write to `$GITHUB_OUTPUT`; it turns out that [output from different jobs in a matrix will override each other](https://github.com/orgs/community/discussions/26639).
Thankfully, [there is an existing action](https://github.com/beacon-biosignals/matrix-output) which is designed to get around this problem by uploading artefacts.)

## Can I run this locally?

Yes, but it's quite tricky.

The easiest thing to do is to run one specific combination of model and adtype.
This doesn't require any environment variables: you can run `julia --project=. main.jl --run <model> <adtype>`.

However, if you want to run the Python script locally, you will need to set the `RESULTS_JSON` environment variable.
The easiest way to do this is to go to the GitHub Actions log for the `collate.py` job, where the value of `RESULTS_JSON` is printed.
It is a multiline string (it's JSON), so you will need to do something like this:

```bash
read -d '' RESULTS_JSON << EOF
(paste the JSON here)
EOF

export RESULTS_JSON
```

Then you can run the Python script locally with `python collate.py`.
It will generate the HTML file in the `html` directory.
