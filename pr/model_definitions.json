{
  "assume_beta": "@model function assume_beta()\n    a ~ Beta(2, 2)\nend\n\n@register assume_beta()",
  "assume_dirichlet": "@model function assume_dirichlet()\n    a ~ Dirichlet([1.0, 5.0])\nend\n\n@register assume_dirichlet()",
  "demo_assume_dot_observe_literal": "@model function demo_assume_dot_observe_literal()\n    # `assume` and literal `dot_observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    [1.5, 2.0] .~ Normal(m, sqrt(s))\nend\n\n@register demo_assume_dot_observe_literal()",
  "demo_assume_index_observe": "@model function demo_assume_index_observe(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `assume` with indexing and `observe`\n    s = TV(undef, length(x))\n    for i in eachindex(s)\n        s[i] ~ InverseGamma(2, 3)\n    end\n    m = TV(undef, length(x))\n    for i in eachindex(m)\n        m[i] ~ Normal(0, sqrt(s[i]))\n    end\n    x ~ MvNormal(m, Diagonal(s))\nend\n\n@register demo_assume_index_observe()",
  "demo_assume_matrix_observe_matrix_index": "@model function demo_assume_matrix_observe_matrix_index(\n    x = transpose([1.5 2.0;]),\n    ::Type{TV} = Array{Float64},\n) where {TV}\n    n = length(x)\n    d = n \u00f7 2\n    s ~ reshape(product_distribution(fill(InverseGamma(2, 3), n)), d, 2)\n    s_vec = vec(s)\n    m ~ MvNormal(zeros(n), Diagonal(s_vec))\n\n    x[:, 1] ~ MvNormal(m, Diagonal(s_vec))\nend\n\n@register demo_assume_matrix_observe_matrix_index()",
  "demo_assume_multivariate_observe": "@model function demo_assume_multivariate_observe(x = [1.5, 2.0])\n    # Multivariate `assume` and `observe`\n    s ~ product_distribution([InverseGamma(2, 3), InverseGamma(2, 3)])\n    m ~ MvNormal(zero(x), Diagonal(s))\n    x ~ MvNormal(m, Diagonal(s))\nend\n@register demo_assume_multivariate_observe()",
  "demo_assume_multivariate_observe_literal": "@model function demo_assume_multivariate_observe_literal()\n    # multivariate `assume` and literal `observe`\n    s ~ product_distribution([InverseGamma(2, 3), InverseGamma(2, 3)])\n    m ~ MvNormal(zeros(2), Diagonal(s))\n    [1.5, 2.0] ~ MvNormal(m, Diagonal(s))\nend\n\n@register demo_assume_multivariate_observe_literal()",
  "demo_assume_observe_literal": "@model function demo_assume_observe_literal()\n    # univariate `assume` and literal `observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    1.5 ~ Normal(m, sqrt(s))\n    2.0 ~ Normal(m, sqrt(s))\nend\n\n@register demo_assume_observe_literal()",
  "demo_assume_submodel_observe_index_literal": "@model function _prior_dot_assume(::Type{TV} = Vector{Float64}) where {TV}\n    s = TV(undef, 2)\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, 2)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    return s, m\nend\n\n@model function demo_assume_submodel_observe_index_literal()\n    # Submodel prior\n    priors ~ to_submodel(_prior_dot_assume(), false)\n    s, m = priors\n    1.5 ~ Normal(m[1], sqrt(s[1]))\n    2.0 ~ Normal(m[2], sqrt(s[2]))\nend\n\n@register demo_assume_submodel_observe_index_literal()",
  "demo_dot_assume_observe": "@model function demo_dot_assume_observe(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and `observe`\n    s = TV(undef, length(x))\n    m = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    x ~ MvNormal(m, Diagonal(s))\nend\n\n@register demo_dot_assume_observe()",
  "demo_dot_assume_observe_index": "@model function demo_dot_assume_observe_index(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and `observe` with indexing\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    for i in eachindex(x)\n        x[i] ~ Normal(m[i], sqrt(s[i]))\n    end\nend\n\n@register demo_dot_assume_observe_index()",
  "demo_dot_assume_observe_index_literal": "@model function demo_dot_assume_observe_index_literal(\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and literal `observe` with indexing\n    s = TV(undef, 2)\n    m = TV(undef, 2)\n    s .~ InverseGamma(2, 3)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n\n    1.5 ~ Normal(m[1], sqrt(s[1]))\n    2.0 ~ Normal(m[2], sqrt(s[2]))\nend\n\n@register demo_dot_assume_observe_index_literal()",
  "assume_lkjcholu": "@model function assume_lkjcholu()\n    a ~ LKJCholesky(5, 1.0, 'U')\nend\n\n@register assume_lkjcholu()",
  "demo_dot_assume_observe_matrix_index": "@model function demo_dot_assume_observe_matrix_index(\n    x = transpose([1.5 2.0;]),\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    x[:, 1] ~ MvNormal(m, Diagonal(s))\nend\n\n@register demo_dot_assume_observe_matrix_index()",
  "demo_dot_assume_observe_submodel": "@model function _likelihood_multivariate_observe(s, m, x)\n    return x ~ MvNormal(m, Diagonal(s))\nend\n\n@model function demo_dot_assume_observe_submodel(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n\n    # Submodel likelihood\n    # With to_submodel, we have to have a left-hand side variable to\n    # capture the result, so we just use a dummy variable\n    _ignore ~ to_submodel(_likelihood_multivariate_observe(s, m, x))\nend\n\n@register demo_dot_assume_observe_submodel()",
  "dot_assume": "@model function dot_assume(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 5)\n    a .~ Normal()\nend\n\n@register dot_assume()",
  "dot_observe": "@model function dot_observe(x = [1.5, 2.0, 2.5])\n    a ~ Normal()\n    x .~ Normal(a)\nend\n\n@register dot_observe()",
  "dppl_gauss_unknown": "n = 10_000\ns = abs(rand()) + 0.5\ny = randn() .+ s * randn(n)\n\n@model function dppl_gauss_unknown(y)\n    N = length(y)\n    m ~ Normal(0, 1)\n    s ~ truncated(Cauchy(0, 5); lower=0)\n    y ~ filldist(Normal(m, s), N)\nend\n\n@register dppl_gauss_unknown(y)",
  "dppl_hier_poisson": "using LazyArrays\nusing Turing: LogPoisson\n\nnd, ns = 5, 10\na0, a1, a0_sig = 1.0, 0.5, 0.3\nn = nd * ns\n# simulate group level parameters\na0s = rand(Normal(0, a0_sig), ns)\nlogpop = rand(Normal(9, 1.5), ns)\n\u03bb = exp.(a0 .+ a0s + (a1 * logpop))\n# and individual data\ny = mapreduce(\u03bbi -> rand(Poisson(\u03bbi), nd), vcat, \u03bb)\nx = repeat(logpop, inner=nd)\nidx = repeat(collect(1:ns), inner=nd)\n\nlazyarray(f, x) = LazyArray(Base.broadcasted(f, x))\n\n@model function dppl_hier_poisson(y, x, idx, ns)\n    a0 ~ Normal(0, 10)\n    a1 ~ Normal(0, 1)\n    a0_sig ~ truncated(Cauchy(0, 1); lower=0)\n    a0s ~ filldist(Normal(0, a0_sig), ns)\n    alpha = a0 .+ a0s[idx] .+ a1 * x\n    y ~ arraydist(lazyarray(LogPoisson, alpha))\nend\n\n@register dppl_hier_poisson(y, x, idx, ns)",
  "dppl_high_dim_gauss": "@model function dppl_high_dim_gauss(D)\n    m ~ filldist(Normal(0, 1), D)\nend\n\n@register dppl_high_dim_gauss(10_000)",
  "dppl_hmm_semisup": "using StatsFuns: logsumexp\n\n# Set up hyperparameters\nK, v, T, T_unsup = 5, 20, 100, 200\nalpha = fill(1.0, K)\nbeta = fill(0.1, v)\ntheta = rand(Dirichlet(alpha), K)\nphi = rand(Dirichlet(beta), K)\n\n# Simulate data (supervised)\nw = Vector{Int}(undef, T)\nz = Vector{Int}(undef, T)\nz[1] = rand(1:K)\nw[1] = rand(Categorical(phi[:, z[1]]))\nfor t in 2:T\n    z[t] = rand(Categorical(theta[:, z[t - 1]]))\n    w[t] = rand(Categorical(phi[:, z[t]]))\nend\n\n# Unsupervised\nu = Vector{Int}(undef, T_unsup)\ny = Vector{Int}(undef, T_unsup)\ny[1] = rand(1:K)\nu[1] = rand(Categorical(phi[:, y[1]]))\nfor t in 2:T_unsup\n    y[t] = rand(Categorical(theta[:, y[t - 1]]))\n    u[t] = rand(Categorical(phi[:, y[t]]))\nend\n\n@model function dppl_hmm_semisup(K, T, T_unsup, w, z, u, alpha, beta)\n    theta ~ filldist(Dirichlet(alpha), K)\n    phi ~ filldist(Dirichlet(beta), K)\n    for t in 1:T\n        w[t] ~ Categorical(phi[:, z[t]]);\n    end\n    for t in 2:T\n        z[t] ~ Categorical(theta[:, z[t - 1]]);\n    end\n\n    TF = eltype(theta)\n    acc = similar(alpha, TF, K)\n    gamma = similar(alpha, TF, K)\n    temp_gamma = similar(alpha, TF, K)\n    for k in 1:K\n        gamma[k] = log(phi[u[1],k])\n    end\n    for t in 2:T_unsup\n        for k in 1:K\n            for j in 1:K\n                acc[j] = gamma[j] + log(theta[k, j]) + log(phi[u[t], k])\n            end\n            temp_gamma[k] = logsumexp(acc)\n        end\n        gamma .= temp_gamma\n    end\n    DynamicPPL.@addlogprob! logsumexp(gamma)\nend\n\n@register dppl_hmm_semisup(K, T, T_unsup, w, z, u, alpha, beta)",
  "dppl_lda": "v = 100      # words\nk = 5        # topics\nm = 10       # number of docs\nalpha = ones(k)\nbeta = ones(v)\n\nphi = rand(Dirichlet(beta), k)\ntheta = rand(Dirichlet(alpha), m)\ndoc_lengths = rand(Poisson(1_000), m)\nn = sum(doc_lengths)\n\nw_lda = Vector{Int}(undef, n)\ndoc_lda = Vector{Int}(undef, n)\nfor i in 1:m\n    # Because all the models exist in the same scope, we need\n    # to add some variable suffixes to avoid local/global \n    # scope warnings. This is quite ugly and should be solved\n    # properly, using e.g. modules or functions.\n    local idx_lda = sum(doc_lengths[1:i-1]) # starting index for inner loop\n    for j in 1:doc_lengths[i]\n        z_lda = rand(Categorical(theta[:, i]))\n        w_lda[idx_lda + j] = rand(Categorical(phi[:, z_lda]))\n        doc_lda[idx_lda + j] = i\n    end\nend\n\n@model function dppl_lda(k, m, w, doc, alpha, beta)\n    theta ~ filldist(Dirichlet(alpha), m)\n    phi ~ filldist(Dirichlet(beta), k)\n    log_phi_dot_theta = log.(phi * theta)\n    DynamicPPL.@addlogprob! sum(log_phi_dot_theta[CartesianIndex.(w, doc)])\nend\n\n@register dppl_lda(k, m, w_lda, doc_lda, alpha, beta)",
  "dppl_logistic_regression": "using StatsFuns: logistic\nusing LazyArrays\n\nd, n = 100, 10_000\nX = randn(d, n)\nw = randn(d)\ny = Int.(logistic.(X' * w) .> 0.5)\n\nfunction safelogistic(x::T) where {T}\n    logistic(x) * (1 - 2 * eps(T)) + eps(T)\nend\n\nlazyarray(f, x) = LazyArray(Base.broadcasted(f, x))\n\n@model dppl_logistic_regression(Xt, y) = begin\n    N, D = size(Xt)\n    w ~ filldist(Normal(), D)\n    y ~ arraydist(lazyarray(x -> Bernoulli(safelogistic(x)), Xt * w))\nend\n\n@register dppl_logistic_regression(X', y)",
  "assume_mvnormal": "@model function assume_mvnormal()\n    a ~ MvNormal([0.0, 0.0], [1.0 0.5; 0.5 1.0])\nend\n\n@register assume_mvnormal()",
  "dppl_sto_volatility": "using DelimitedFiles: readdlm\n\npath = \"$(@__DIR__)/../data/dppl_sto_volatility.csv\"\ndata, _ = readdlm(path, ',', header=true)\nto_num(x) = x isa Number ? x : 0.1\ny = map(to_num, data[1:500,2])\n\n@model dppl_sto_volatility(y, ::Type{Tv}=Vector{Float64}) where {Tv} = begin\n    T = length(y)\n    \u03bc ~ Cauchy(0, 10)\n    \u03d5 ~ Uniform(-1, 1)\n    \u03c3 ~ truncated(Cauchy(0, 5); lower=0)\n\n    h = Tv(undef, T)\n    h[1] ~ Normal(\u03bc, \u03c3 / sqrt(1 - \u03d5^2))\n    y[1] ~ Normal(0, exp(h[1] / 2))\n    for t in 2:T\n        h[t] ~ Normal(\u03bc + \u03d5 * (h[t - 1] - \u03bc), \u03c3)\n        y[t] ~ Normal(0, exp(h[t] / 2))\n    end\nend\n\n@register dppl_sto_volatility(y)",
  "dynamic_constraint": "@model function dynamic_constraint()\n    a ~ Normal()\n    b ~ truncated(Normal(); lower = a)\nend\n\n@register dynamic_constraint()",
  "multiple_constraints_same_var": "@model function multiple_constraints_same_var(::Type{TV} = Vector{Float64}) where {TV}\n    x = TV(undef, 5)\n    x[1] ~ Normal()\n    x[2] ~ InverseGamma(2, 3)\n    x[3] ~ truncated(Normal(), -5, 20)\n    x[4:5] ~ Dirichlet([1.0, 2.0])\nend\n\n@register multiple_constraints_same_var()",
  "multithreaded": "#=\nMost models in ADTests are run with 1 thread. This model is run with 2 threads\nto properly demonstrate the compatibility with multithreaded observe\nstatements. See `main.jl` for more information.\n=#\n\n@model function multithreaded(x)\n    a ~ Normal()\n    Threads.@threads for i in eachindex(x)\n        x[i] ~ Normal(a)\n    end\nend\n\n@register multithreaded([1.5, 2.0, 2.5, 1.5, 2.0, 2.5])",
  "n010": "@model function n010(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 10)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\n@register n010()",
  "n050": "@model function n050(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 50)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\n@register n050()",
  "n100": "@model function n100(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 100)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\n@register n100()",
  "n500": "@model function n500(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 500)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\n@register n500()",
  "observe_bernoulli": "@model function observe_bernoulli(x = [true, false, true])\n    a ~ Beta(2, 2)\n    for i in eachindex(x)\n        x[i] ~ Bernoulli(a)\n    end\nend\n\n@register observe_bernoulli()",
  "observe_categorical": "@model function observe_categorical(x = [1, 2, 1, 2, 2])\n    a ~ Dirichlet(2, 3)\n    for i in eachindex(x)\n        x[i] ~ Categorical(a)\n    end\nend\n\n@register observe_categorical()",
  "assume_normal": "@model function assume_normal()\n    a ~ Normal()\nend\n\n@register assume_normal()",
  "observe_index": "@model function observe_index(x = [1.5, 2.0, 2.5])\n    a ~ Normal()\n    for i in eachindex(x)\n        x[i] ~ Normal(a)\n    end\nend\n\n@register observe_index()",
  "observe_literal": "@model function observe_literal()\n    a ~ Normal()\n    1.5 ~ Normal(a)\nend\n\n@register observe_literal()",
  "observe_multivariate": "@model function observe_multivariate(\n    x = [1.5, 2.0, 2.5],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    a = TV(undef, length(x))\n    a .~ Normal()\n    x ~ MvNormal(a, I)\nend\n\n@register observe_multivariate()",
  "observe_submodel": "@model function inner2(x, a)\n    x ~ Normal(a)\nend\n@model function observe_submodel(x = 1.5)\n    a ~ Normal()\n    _ignore ~ to_submodel(inner2(x, a))\nend\n\n@register observe_submodel()",
  "pdb_eight_schools_centered": "J = 8\ny = [28, 8, -3, 7, -1, 1, 18, 12]\nsigma = [15, 10, 16, 11, 9, 11, 10, 18]\n\n@model function pdb_eight_schools_centered(J, y, sigma)\n    mu ~ Normal(0, 5)\n    tau ~ truncated(Cauchy(0, 5); lower = 0)\n    theta = Vector{Float64}(undef, J)\n    for i = 1:J\n        theta[i] ~ Normal(mu, tau)\n        y[i] ~ Normal(theta[i], sigma[i])\n    end\nend\n\n@register pdb_eight_schools_centered(J, y, sigma)",
  "pdb_eight_schools_noncentered": "J = 8\ny = [28, 8, -3, 7, -1, 1, 18, 12]\nsigma = [15, 10, 16, 11, 9, 11, 10, 18]\n\n@model function pdb_eight_schools_noncentered(J, y, sigma)\n    mu ~ Normal(0, 5)\n    tau ~ truncated(Cauchy(0, 5); lower = 0)\n    theta_trans = Vector{Float64}(undef, J)\n    for i = 1:J\n        theta_trans[i] ~ Normal(0, 1)\n        theta = theta_trans[i] * tau + mu\n        y[i] ~ Normal(theta, sigma[i])\n    end\nend\n\n@register pdb_eight_schools_noncentered(J, y, sigma)",
  "von_mises": "@model function von_mises(x)\n    a ~ InverseGamma(2, 3)\n    x ~ VonMises(0, a)\nend\n\n@register von_mises(0.4)",
  "assume_submodel": "@model function inner1()\n    return a ~ Normal()\nend\n@model function assume_submodel()\n    a ~ to_submodel(inner1())\n    x ~ Normal(a)\nend\n\n@register assume_submodel()",
  "assume_wishart": "@model function assume_wishart()\n    a ~ Wishart(7, [1.0 0.5; 0.5 1.0])\nend\n\n@register assume_wishart()",
  "broadcast_macro": "@model function broadcast_macro(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    a ~ Normal(0, 1)\n    b ~ InverseGamma(2, 3)\n    @. x ~ Normal(a, $(sqrt(b)))\nend\n\n@register broadcast_macro()",
  "control_flow": "#= \nThis model illustrates dynamic control flow inside a model that depends on the\nvalue of a random variable. This will cause problems with ReverseDiff's\ncompiled tapes, as a tape compiled at a given value of `a` may not be\nappropriate for a different value of `a`.\n\nTo make sure that the table correctly reflects this issue, the preparation for\nthe gradient is carried out at a value of `a > 0`, and the gradient is\nevaluated at a value of `a < 0`. See `main.jl` for more information.\n=#\n\n@model function control_flow()\n    a ~ Normal()\n    if a > 0\n        b ~ Normal()\n    else\n        b ~ Beta(2, 2)\n    end\nend\n\n@register control_flow()",
  "demo_assume_dot_observe": "@model function demo_assume_dot_observe(x = [1.5, 2.0])\n    # `assume` and `dot_observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    x .~ Normal(m, sqrt(s))\nend\n\n@register demo_assume_dot_observe()"
}